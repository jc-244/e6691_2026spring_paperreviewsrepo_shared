{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e32154c7",
      "metadata": {},
      "source": [
        "# `F.multi_head_attention_forward`\n",
        "\n",
        "Source (pytorch/torch/nn/functional.py):\n",
        "- https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py#L6244-L6695"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3432333",
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_head_attention_forward(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    embed_dim_to_check: int,\n",
        "    num_heads: int,\n",
        "    in_proj_weight: Optional[Tensor],\n",
        "    in_proj_bias: Optional[Tensor],\n",
        "    bias_k: Optional[Tensor],\n",
        "    bias_v: Optional[Tensor],\n",
        "    add_zero_attn: bool,\n",
        "    dropout_p: float,\n",
        "    out_proj_weight: Tensor,\n",
        "    out_proj_bias: Optional[Tensor],\n",
        "    training: bool = True,\n",
        "    key_padding_mask: Optional[Tensor] = None,\n",
        "    need_weights: bool = True,\n",
        "    attn_mask: Optional[Tensor] = None,\n",
        "    use_separate_proj_weight: bool = False,\n",
        "    q_proj_weight: Optional[Tensor] = None,\n",
        "    k_proj_weight: Optional[Tensor] = None,\n",
        "    v_proj_weight: Optional[Tensor] = None,\n",
        "    static_k: Optional[Tensor] = None,\n",
        "    static_v: Optional[Tensor] = None,\n",
        "    average_attn_weights: bool = True,\n",
        "    is_causal: bool = False,\n",
        ") -> tuple[Tensor, Optional[Tensor]]:\n",
        "    tens_ops = (\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        in_proj_weight,\n",
        "        in_proj_bias,\n",
        "        bias_k,\n",
        "        bias_v,\n",
        "        out_proj_weight,\n",
        "        out_proj_bias,\n",
        "    )\n",
        "    if has_torch_function(tens_ops):\n",
        "        return handle_torch_function(\n",
        "            multi_head_attention_forward,\n",
        "            tens_ops,\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            embed_dim_to_check,\n",
        "            num_heads,\n",
        "            in_proj_weight,\n",
        "            in_proj_bias,\n",
        "            bias_k,\n",
        "            bias_v,\n",
        "            add_zero_attn,\n",
        "            dropout_p,\n",
        "            out_proj_weight,\n",
        "            out_proj_bias,\n",
        "            training=training,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=need_weights,\n",
        "            attn_mask=attn_mask,\n",
        "            is_causal=is_causal,\n",
        "            use_separate_proj_weight=use_separate_proj_weight,\n",
        "            q_proj_weight=q_proj_weight,\n",
        "            k_proj_weight=k_proj_weight,\n",
        "            v_proj_weight=v_proj_weight,\n",
        "            static_k=static_k,\n",
        "            static_v=static_v,\n",
        "            average_attn_weights=average_attn_weights,\n",
        "        )\n",
        "\n",
        "    is_batched = _mha_shape_check(\n",
        "        query, key, value, key_padding_mask, attn_mask, num_heads\n",
        "    )\n",
        "\n",
        "    # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\n",
        "    # is batched, run the computation and before returning squeeze the\n",
        "    # batch dimension so that the output doesn't carry this temporary batch dimension.\n",
        "    if not is_batched:\n",
        "        # unsqueeze if the input is unbatched\n",
        "        query = query.unsqueeze(1)\n",
        "        key = key.unsqueeze(1)\n",
        "        value = value.unsqueeze(1)\n",
        "        if key_padding_mask is not None:\n",
        "            key_padding_mask = key_padding_mask.unsqueeze(0)\n",
        "\n",
        "    # set up shape vars\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "\n",
        "    key_padding_mask = _canonical_mask(\n",
        "        mask=key_padding_mask,\n",
        "        mask_name=\"key_padding_mask\",\n",
        "        other_type=_none_or_dtype(attn_mask),\n",
        "        other_name=\"attn_mask\",\n",
        "        target_type=query.dtype,\n",
        "    )\n",
        "\n",
        "    if is_causal and attn_mask is None:\n",
        "        raise RuntimeError(\n",
        "            \"Need attn_mask if specifying the is_causal hint. \"\n",
        "            \"You may use the Transformer module method \"\n",
        "            \"`generate_square_subsequent_mask` to create this mask.\"\n",
        "        )\n",
        "\n",
        "    if is_causal and key_padding_mask is None and not need_weights:\n",
        "        # when we have a kpm or need weights, we need attn_mask\n",
        "        # Otherwise, we use the is_causal hint go as is_causal\n",
        "        # indicator to SDPA.\n",
        "        attn_mask = None\n",
        "    else:\n",
        "        attn_mask = _canonical_mask(\n",
        "            mask=attn_mask,\n",
        "            mask_name=\"attn_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=query.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "        if key_padding_mask is not None:\n",
        "            # We have the attn_mask, and use that to merge kpm into it.\n",
        "            # Turn off use of is_causal hint, as the merged mask is no\n",
        "            # longer causal.\n",
        "            is_causal = False\n",
        "\n",
        "    if embed_dim != embed_dim_to_check:\n",
        "        raise AssertionError(\n",
        "            f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n",
        "        )\n",
        "    if isinstance(embed_dim, torch.Tensor):\n",
        "        # embed_dim can be a tensor when JIT tracing\n",
        "        head_dim = embed_dim.div(num_heads, rounding_mode=\"trunc\")\n",
        "    else:\n",
        "        head_dim = embed_dim // num_heads\n",
        "    if head_dim * num_heads != embed_dim:\n",
        "        raise AssertionError(\n",
        "            f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
        "        )\n",
        "    if use_separate_proj_weight:\n",
        "        # allow MHA to have different embedding dimensions when separate projection weights are used\n",
        "        if key.shape[:2] != value.shape[:2]:\n",
        "            raise AssertionError(\n",
        "                f\"key's sequence and batch dims {key.shape[:2]} do not match value's {value.shape[:2]}\"\n",
        "            )\n",
        "    else:\n",
        "        if key.shape != value.shape:\n",
        "            raise AssertionError(\n",
        "                f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
        "            )\n",
        "\n",
        "    #\n",
        "    # compute in-projection\n",
        "    #\n",
        "    if not use_separate_proj_weight:\n",
        "        if in_proj_weight is None:\n",
        "            raise AssertionError(\n",
        "                \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
        "            )\n",
        "        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
        "    else:\n",
        "        if q_proj_weight is None:\n",
        "            raise AssertionError(\n",
        "                \"use_separate_proj_weight is True but q_proj_weight is None\"\n",
        "            )\n",
        "        if k_proj_weight is None:\n",
        "            raise AssertionError(\n",
        "                \"use_separate_proj_weight is True but k_proj_weight is None\"\n",
        "            )\n",
        "        if v_proj_weight is None:\n",
        "            raise AssertionError(\n",
        "                \"use_separate_proj_weight is True but v_proj_weight is None\"\n",
        "            )\n",
        "        if in_proj_bias is None:\n",
        "            b_q = b_k = b_v = None\n",
        "        else:\n",
        "            b_q, b_k, b_v = in_proj_bias.chunk(3)\n",
        "        q, k, v = _in_projection(\n",
        "            query,\n",
        "            key,\n",
        "            value,\n",
        "            q_proj_weight,\n",
        "            k_proj_weight,\n",
        "            v_proj_weight,\n",
        "            b_q,\n",
        "            b_k,\n",
        "            b_v,\n",
        "        )\n",
        "\n",
        "    # prep attention mask\n",
        "\n",
        "    if attn_mask is not None:\n",
        "        # ensure attn_mask's dim is 3\n",
        "        if attn_mask.dim() == 2:\n",
        "            correct_2d_size = (tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_2d_size:\n",
        "                raise RuntimeError(\n",
        "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n",
        "                )\n",
        "            attn_mask = attn_mask.unsqueeze(0)\n",
        "        elif attn_mask.dim() == 3:\n",
        "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
        "            if attn_mask.shape != correct_3d_size:\n",
        "                raise RuntimeError(\n",
        "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n",
        "                )\n",
        "        else:\n",
        "            raise RuntimeError(\n",
        "                f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n",
        "            )\n",
        "\n",
        "    # add bias along batch dimension (currently second)\n",
        "    if bias_k is not None and bias_v is not None:\n",
        "        if static_k is not None:\n",
        "            raise AssertionError(\"bias cannot be added to static key.\")\n",
        "        if static_v is not None:\n",
        "            raise AssertionError(\"bias cannot be added to static value.\")\n",
        "        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
        "        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
        "        if attn_mask is not None:\n",
        "            # pyrefly: ignore [bad-argument-type]\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            # pyrefly: ignore [bad-argument-type]\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "    else:\n",
        "        if bias_k is not None:\n",
        "            raise AssertionError(\"bias_k is set but bias_v is None\")\n",
        "        if bias_v is not None:\n",
        "            raise AssertionError(\"bias_v is set but bias_k is None\")\n",
        "\n",
        "    #\n",
        "    # reshape q, k, v for multihead attention and make them batch first\n",
        "    #\n",
        "    # pyrefly: ignore [no-matching-overload]\n",
        "    q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    if static_k is None:\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        if static_k.size(0) != bsz * num_heads:\n",
        "            raise AssertionError(\n",
        "                f\"expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}\"\n",
        "            )\n",
        "        if static_k.size(2) != head_dim:\n",
        "            raise AssertionError(\n",
        "                f\"expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}\"\n",
        "            )\n",
        "        k = static_k\n",
        "    if static_v is None:\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
        "    else:\n",
        "        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n",
        "        if static_v.size(0) != bsz * num_heads:\n",
        "            raise AssertionError(\n",
        "                f\"expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}\"\n",
        "            )\n",
        "        if static_v.size(2) != head_dim:\n",
        "            raise AssertionError(\n",
        "                f\"expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}\"\n",
        "            )\n",
        "        v = static_v\n",
        "\n",
        "    # add zero attention along batch dimension (now first)\n",
        "    if add_zero_attn:\n",
        "        zero_attn_shape = (bsz * num_heads, 1, head_dim)\n",
        "        k = torch.cat(\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            [k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)],\n",
        "            dim=1,\n",
        "        )\n",
        "        v = torch.cat(\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            [v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)],\n",
        "            dim=1,\n",
        "        )\n",
        "        if attn_mask is not None:\n",
        "            # pyrefly: ignore [bad-argument-type]\n",
        "            attn_mask = pad(attn_mask, (0, 1))\n",
        "        if key_padding_mask is not None:\n",
        "            # pyrefly: ignore [bad-argument-type]\n",
        "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
        "\n",
        "    # update source sequence length after adjustments\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    # merge key padding and attention masks\n",
        "    if key_padding_mask is not None:\n",
        "        if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
        "            _check_key_padding_mask(key_padding_mask, src_len, bsz)\n",
        "\n",
        "        key_padding_mask = (\n",
        "            key_padding_mask.view(bsz, 1, 1, src_len)\n",
        "            .expand(-1, num_heads, -1, -1)\n",
        "            .reshape(bsz * num_heads, 1, src_len)\n",
        "        )\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        else:\n",
        "            attn_mask = attn_mask + key_padding_mask\n",
        "\n",
        "    # adjust dropout probability\n",
        "    if not training:\n",
        "        dropout_p = 0.0\n",
        "\n",
        "    #\n",
        "    # (deep breath) calculate attention and out projection\n",
        "    #\n",
        "\n",
        "    if need_weights:\n",
        "        _B, _Nt, E = q.shape\n",
        "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
        "\n",
        "        if is_causal and attn_mask is None:\n",
        "            raise AssertionError(\"FIXME: is_causal not implemented for need_weights\")\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_output_weights = torch.baddbmm(\n",
        "                attn_mask, q_scaled, k.transpose(-2, -1)\n",
        "            )\n",
        "        else:\n",
        "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
        "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "        if dropout_p > 0.0:\n",
        "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
        "\n",
        "        attn_output = torch.bmm(attn_output_weights, v)\n",
        "\n",
        "        attn_output = (\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
        "        )\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "\n",
        "        # optionally average attention weights over heads\n",
        "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
        "        if average_attn_weights:\n",
        "            attn_output_weights = attn_output_weights.mean(dim=1)\n",
        "\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "            attn_output_weights = attn_output_weights.squeeze(0)\n",
        "        return attn_output, attn_output_weights\n",
        "    else:\n",
        "        # attn_mask can be either (L,S) or (N*num_heads, L, S)\n",
        "        # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)\n",
        "        # in order to match the input for SDPA of (N, num_heads, L, S)\n",
        "        if attn_mask is not None:\n",
        "            if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
        "                attn_mask = attn_mask.unsqueeze(0)\n",
        "            else:\n",
        "                attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)\n",
        "\n",
        "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
        "        # pyrefly: ignore [no-matching-overload]\n",
        "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
        "\n",
        "        attn_output = scaled_dot_product_attention(\n",
        "            q, k, v, attn_mask, dropout_p, is_causal\n",
        "        )\n",
        "        attn_output = (\n",
        "            # pyrefly: ignore [no-matching-overload]\n",
        "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "        )\n",
        "\n",
        "        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
        "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "        if not is_batched:\n",
        "            # squeeze the output if input was unbatched\n",
        "            attn_output = attn_output.squeeze(1)\n",
        "        return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a6d7c9",
      "metadata": {},
      "source": [
        "## Excerpt: three masking controls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73b5efb",
      "metadata": {},
      "outputs": [],
      "source": [
        "    key_padding_mask = _canonical_mask(\n",
        "        mask=key_padding_mask,\n",
        "    )\n",
        "    if is_causal and attn_mask is None:\n",
        "        raise RuntimeError(\n",
        "            \"Need attn_mask if specifying the is_causal hint. \"\n",
        "        )\n",
        "    if is_causal and key_padding_mask is None and not need_weights:\n",
        "        attn_mask = None\n",
        "    else:\n",
        "        attn_mask = _canonical_mask(\n",
        "            mask=attn_mask,\n",
        "        )\n",
        "        if key_padding_mask is not None:\n",
        "            is_causal = False\n",
        "    if key_padding_mask is not None:\n",
        "        if attn_mask is None:\n",
        "            attn_mask = key_padding_mask\n",
        "        else:\n",
        "            attn_mask = attn_mask + key_padding_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3dfe1b0",
      "metadata": {},
      "source": [
        "## Excerpt: math implementation or CUDA-optimized SDPA kernels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f7b0cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "    if need_weights:\n",
        "        # q_scaled = q / sqrt(E)\n",
        "        q_scaled = q * math.sqrt(1.0 / float(E))\n",
        "        if attn_mask is not None:\n",
        "            attn_output_weights = torch.baddbmm(\n",
        "                attn_mask, q_scaled, k.transpose(-2, -1)\n",
        "            )\n",
        "        else:\n",
        "            attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
        "        # A = softmax((QK^T + mask) / sqrt(E))\n",
        "        attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
        "        if dropout_p > 0.0:\n",
        "            attn_output_weights = dropout(attn_output_weights, p=dropout_p)\n",
        "        # O = A V\n",
        "        attn_output = torch.bmm(attn_output_weights, v)\n",
        "        return attn_output, attn_output_weights\n",
        "    else:\n",
        "        attn_output = scaled_dot_product_attention(\n",
        "            q, k, v, attn_mask, dropout_p, is_causal\n",
        "        )\n",
        "        return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83fe79c5",
      "metadata": {},
      "source": [
        "## Supplement: SDPA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f561009b",
      "metadata": {},
      "source": [
        "Source (pytorch/aten/src/ATen/native/transformers\n",
        "/attention.cpp):\n",
        "- https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/attention.cpp#L718C1-L848"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17fbfd82",
      "metadata": {},
      "source": [
        "### Pseudocode\n",
        "```\n",
        "function scaled_dot_product_attention(query, key, value, attn_mask, dropout_p, is_causal, scale, enable_gqa):\n",
        "    \n",
        "    # Step 1: Select backend\n",
        "    backend ‚Üê choose_best_backend(device, inputs)\n",
        "    \n",
        "    # Step 2: Dispatch to backend\n",
        "    switch backend:\n",
        "        case CUDNN:\n",
        "            return cudnn_sdpa(query, key, value, attn_mask, ...)\n",
        "        case FLASH:\n",
        "            return flash_sdpa(query, key, value, ...)\n",
        "        case EFFICIENT:\n",
        "            return efficient_sdpa(query, key, value, attn_mask, ...)\n",
        "        case MATH:\n",
        "            return math_sdpa(query, key, value, attn_mask, ...)  // fallback\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
